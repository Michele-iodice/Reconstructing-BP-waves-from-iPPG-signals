### DATASET ###
#
# dataset: name of the module that implement the pyVHR.datasets.Dataset class. The class and the module must have the same name!
# path: path to the module where 'dataset' is defined. If 'None' then the dataset is searched inside the package datasets dir "pyVHR/datasets/".
# videodataDIR: path to the video directory of the dataset.
# BVPdataDIR: path to the ground truth BVP of the dataset.
#
[DATASET]
dataset = bp4d
path = C:/Users/Utente/Documents/GitHub/Reconstructing-BP-waves-from-iPPG-signals/scripts/python/dataset
videodataDIR = D:/datasetBP4D+
BVPdataDIR = D:/datasetBP4D+

### SIGNAL ###
#
# target_device: type of device to be use CPU or GPU.
# method: method of parsing the video.
# approach: method of skin extraction from the signal.
# winsize: size of the slide window of the signal.
# minHz, maxHz: min and max frequency of the bandpass filter
# Skin_LOW_TH: int RGB color low threshold used for extracting the skin. This means, colors below RGB [V,V,V] are excluded, where V is the chosen int value.
# Skin_HIGH_TH: int RGB color high threshold used for extracting the skin. This means, colors above RGB [V,V,V] are excluded, where V is the int chosen value.
# RGB_LOW_TH: int RGB color low threshold used for extracting the signal. This means, ROI colors below RGB [V,V,V] are excluded, where V is the chosen int value.
# RGB_HIGH_TH: int RGB color high threshold used for extracting the signal. This means, ROI colors above RGB [V,V,V] are excluded, where V is the chosen int value.
# SIG_SampleRate: sample rate of the ground truth signal.
# frameRate: sample rate of the video registered.
#
[Sig]
target_device = CPU
method= faceparsing
approach= holistic
winsize = 15
minHz = 0.5
maxHz = 6.0
RGB_LOW_TH = 75
RGB_HIGH_TH = 230
Skin_LOW_TH = 75
Skin_HIGH_TH = 230
SIG_SampleRate = 1000
frameRate = 250

### PTT ###
#
# f1: min frequency range for the systolic detection algorithm over the database.
# f2: max frequency range for the systolic detection algorithm over the database.
# w1: represents the window size of the systolic-peak duration.
# w2: represents the window size of approximately one beat duration.
# beta: is the offset level.
# threshold: is the window size threshold for the ptt detection.
#
[PTT]
f1 = 0.5
f2 = 8.0
w1 = 111
w2 = 667
beta = 0.02
threshold = 40

### PTT ###
#
# n_estimators:  The number of trees in the forest.
# random_state: The seed that is used by the algorithm during the training process.
# criterion: The function to measure the quality of a split.
# max_depth: represent the max depth of each decision tree in the forest.
#
[RFModel]
n_estimators = 100
random_state = 42
criterion = absolute_error
max_depth = 10

###U-net configuration parameter###
#
# back_size: back size of the model used
# epochs: number of epoch to use to training
# verbose: if it is true the process line of the training is shown
# data_path: path where to save the data
# cardinality: number of splitting path of the resnet block
# n_block 1/2/3/4: number of resnet blocks for each group
# output_channels: output channels of the decoder blocks
# backbone_name: nome of backbone to use
# pretrained: if it is true the backbone is pre-trained with imagenet weights
# freeze_backbone: if it is true the top levels of the backbone are frozen
# checkpoint_path: path where to save the checkpoint
# model_path: path where to save the model
[UnetParameter]
frameRate = 100
BATCH_SIZE = 16
EPOCHS = 50
VERBOSE = true
data_path = ../dataset/data.h5
cardinality = 32
n_blocks1 = 3
n_blocks2 = 4
n_blocks3 =23
n_blocks4 = 3
output_channels = 256, 128, 64, 32, 16
backbone_name = resnext101_32x8d
pretrained= true
freeze_backbone = true
checkpoint_path = ../train/result/weights.pth
model_path = ../train/result/model.json